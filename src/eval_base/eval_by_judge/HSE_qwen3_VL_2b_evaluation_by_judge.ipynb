{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)]()"
      ],
      "metadata": {
        "id": "teZjja4brbEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai -q"
      ],
      "metadata": {
        "id": "Y0UPI7Cjvjms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading the dataset with base model answers"
      ],
      "metadata": {
        "id": "jRmWIJbluuGN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjSIDh85g0to",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7edab898-54c0-4fe4-d905-2ba82467b812"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"leinms/flickr30k-qwen3vl-baseline\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Judge LLM prompt for evaluation"
      ],
      "metadata": {
        "id": "Vy3PQklpu2EA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "You are an evaluation model. Your task is to score a candidate image caption (“model_answ”)\n",
        "based on (1) the input image and (2) the gold caption (“golden_answer”).\n",
        "You must evaluate the candidate caption strictly according to the criteria described below.\n",
        "Your output MUST follow the ResponseStructure schema and must contain only valid integers.\n",
        "\n",
        "----------------------------------------------------------------------\n",
        "EVALUATION CRITERIA (0–10 each, except final_score):\n",
        "\n",
        "1. semantic_similarity (0–10)\n",
        "   - Measures how close the meaning of model_answ is to golden_answer.\n",
        "   - 0 = completely different meaning\n",
        "   - 5 = partially overlapping meaning, some correct concepts\n",
        "   - 10 = nearly identical meaning, all major ideas align\n",
        "\n",
        "2. object_coverage (0–10)\n",
        "   - Evaluates how well the candidate caption covers the key objects and entities\n",
        "     that appear in the image AND are mentioned in the golden caption.\n",
        "   - 0 = covers none of the important objects\n",
        "   - 10 = fully covers all important objects\n",
        "\n",
        "3. hallucination_score (0–10)\n",
        "   - Measures the absence of hallucinations (objects or facts NOT present in the image).\n",
        "   - 0 = many hallucinations\n",
        "   - 5 = small mistakes or 1–2 minor hallucinated details\n",
        "   - 10 = no hallucinated objects or false claims\n",
        "\n",
        "4. completeness (0–10)\n",
        "   - Checks whether the model caption includes all essential elements needed\n",
        "     to describe the scene similarly to the golden caption.\n",
        "   - 0 = major omissions\n",
        "   - 10 = fully complete, nothing important missing\n",
        "\n",
        "5. instruction_following (0–10)\n",
        "   - Measures whether the model caption follows the format and intent\n",
        "     implied by the golden caption (e.g., factual description, no storytelling,\n",
        "     no added opinions, no unnecessary style).\n",
        "   - 0 = fails to follow basic instructions\n",
        "   - 10 = fully consistent with the expected style and constraints\n",
        "\n",
        "6. fluency (0–10)\n",
        "   - Measures textual quality: clarity, grammar, readability, and natural flow.\n",
        "   - 0 = hard to read, ungrammatical\n",
        "   - 10 = fluent, well-formed, natural English\n",
        "\n",
        "7. final_score (0–100)\n",
        "   - An overall integrated score.\n",
        "   - Must be a reasonable weighted aggregation of the above criteria.\n",
        "   - You decide the weighting, but it must correlate with quality.\n",
        "   - 0 = unusable caption\n",
        "   - 100 = excellent caption, nearly perfect\n",
        "\n",
        "----------------------------------------------------------------------\n",
        "ADDITIONAL RULES:\n",
        "\n",
        "- Rely on the IMAGE first, and the golden caption second.\n",
        "- Be strict: do NOT inflate scores.\n",
        "- Respond ONLY with valid ResponseStructure JSON fields.\n",
        "- Do NOT include any explanation, comments, or extra text.\n",
        "\n",
        "----------------------------------------------------------------------\n",
        "INPUT PROVIDED:\n",
        "- An image (\"image_url\")\n",
        "- golden_answer: the correct caption\n",
        "- model_answ: the caption to evaluate\n",
        "\n",
        "Evaluate the model caption and produce numeric scores for all fields.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "44ALnhODAlHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import base64\n",
        "from io import BytesIO\n",
        "from datasets import load_dataset\n",
        "from openai import OpenAIb\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "client = OpenAI(api_key=\"YOUR_API_KEY\")\n",
        "\n",
        "class ResponseStructure(BaseModel):\n",
        "    # 0–10\n",
        "    semantic_similarity: int = Field(..., ge=0, le=10, description=\"Семантическая близость к золотому описанию\")\n",
        "    object_coverage: int = Field(..., ge=0, le=10, description=\"Покрытие важных объектов\")\n",
        "    hallucination_score: int = Field(..., ge=0, le=10, description=\"Отсутствие галлюцинаций (10 = нет ошибок)\")\n",
        "    completeness: int = Field(..., ge=0, le=10, description=\"Полнота описания\")\n",
        "    instruction_following: int = Field(..., ge=0, le=10, description=\"Следование формату/инструкции\")\n",
        "    fluency: int = Field(..., ge=0, le=10, description=\"Связность и грамотность текста\")\n",
        "    final_score: int = Field(..., ge=0, le=100, description=\"Итоговый интегральный балл\")\n",
        "\n",
        "\n",
        "def pil_image_to_data_url(pil_img, fmt=\"JPEG\"):\n",
        "    buf = BytesIO()\n",
        "    pil_img.save(buf, format=fmt)\n",
        "    buf.seek(0)\n",
        "    img_bytes = buf.read()\n",
        "    img_b64 = base64.b64encode(img_bytes).decode(\"utf-8\")\n",
        "    mime = \"image/jpeg\" if fmt.lower() in (\"jpeg\", \"jpg\") else f\"image/{fmt.lower()}\"\n",
        "    return f\"data:{mime};base64,{img_b64}\"\n",
        "\n",
        "\n",
        "# --- один вызов модели для одного примера датасета ---\n",
        "\n",
        "def run_model_for_example(img, alt_text, baseline_answ):\n",
        "    data_url = pil_image_to_data_url(img)\n",
        "\n",
        "    response = client.responses.parse(\n",
        "        model=\"gpt-4o-2024-08-06\",\n",
        "        instructions=prompt,\n",
        "        text_format=ResponseStructure,\n",
        "        input=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"type\": \"input_text\",\n",
        "                        \"text\": (\n",
        "                            f\"golden_answer: {alt_text}\\n\"\n",
        "                            f\"model_answ: {baseline_answ}\\n\"\n",
        "                        ),\n",
        "                    },\n",
        "                    {\n",
        "                        \"type\": \"input_image\",\n",
        "                        \"image_url\": data_url,\n",
        "                    },\n",
        "                ],\n",
        "\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.output_parsed"
      ],
      "metadata": {
        "id": "XSy9QVsQg8U9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = dataset[\"test\"].select_columns([\"image\", \"alt_text\", \"baseline_answ\"])\n",
        "\n",
        "ex = ds[0]\n",
        "answer = run_model_for_example(ex[\"image\"], ex[\"alt_text\"], ex[\"baseline_answ\"])\n",
        "print(answer)\n"
      ],
      "metadata": {
        "id": "Is-PdYqCw8t9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "ds = dataset[\"test\"].select_columns([\"image\", \"alt_text\", \"baseline_answ\"])\n",
        "\n",
        "all_outputs = []\n",
        "\n",
        "\n",
        "for ex in tqdm(ds, total=len(ds), desc=\"Processing dataset\"):\n",
        "    try:\n",
        "        ans = run_model_for_example(\n",
        "            ex[\"image\"],\n",
        "            ex[\"alt_text\"],\n",
        "            ex[\"baseline_answ\"],\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка на примере {i}: {e}\")\n",
        "        ans = None\n",
        "\n",
        "    all_outputs.append(ans)\n"
      ],
      "metadata": {
        "id": "VNeHgVxig8fj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da6fa00f-7163-4f5d-9571-7d0cc66bb895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing dataset: 100%|██████████| 1000/1000 [56:46<00:00,  3.41s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KJNkIF-pZ8lA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}